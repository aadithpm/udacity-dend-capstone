{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### US Demographic and Immigration Data\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The goal of this project is to construct a pipeline for building a data lake in S3 with Spark for demographic and immigration data in the US. Three data sources are used:\n",
    "\n",
    "* I94 Immigration Data\n",
    "* USA Airport Codes\n",
    "* USA City Demographic Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import pandas\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://udacity-data/capstone/immigration/sas_data/, s3a://udacity-data/capstone/immigration/I94_SAS_LABELS_Descriptions.SAS\n",
      "s3a://udacity-data/capstone/airport/airport-codes_csv.csv\n",
      "s3a://udacity-data/capstone/demographic/us-cities-demographic.csv\n",
      "s3a://udacity-data/capstone/output\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read_file(open('config.cfg'))\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "bucket = config['S3']['BUCKET']\n",
    "immigration_path = bucket + config['S3']['IMMIGRATION_DATA']\n",
    "immigration_labels = bucket + config['S3']['IMMIGRATION_LABELS']\n",
    "airport_path = bucket + config['S3']['AIRPORT_CODES_DATA']\n",
    "city_path = bucket + config['S3']['CITY_DATA']\n",
    "output_path = bucket + config['S3']['OUTPUT']\n",
    "\n",
    "\n",
    "print(f'{immigration_path}, {immigration_labels}')\n",
    "print(f'{airport_path}')\n",
    "print(f'{city_path}')\n",
    "print(f'{output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON']='/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']='/usr/bin/python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.74:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6ba4ce0ef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.config('spark.jars.packages', \n",
    "                                    'org.apache.hadoop:hadoop-aws:2.7.0').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",\n",
    "                                     os.environ['AWS_ACCESS_KEY_ID'])\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \n",
    "                                     os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data = spark.read.parquet(immigration_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " cicid    | 5748517.0      \n",
      " i94yr    | 2016.0         \n",
      " i94mon   | 4.0            \n",
      " i94cit   | 245.0          \n",
      " i94res   | 438.0          \n",
      " i94port  | LOS            \n",
      " arrdate  | 20574.0        \n",
      " i94mode  | 1.0            \n",
      " i94addr  | CA             \n",
      " depdate  | 20582.0        \n",
      " i94bir   | 40.0           \n",
      " i94visa  | 1.0            \n",
      " count    | 1.0            \n",
      " dtadfile | 20160430       \n",
      " visapost | SYD            \n",
      " occup    | null           \n",
      " entdepa  | G              \n",
      " entdepd  | O              \n",
      " entdepu  | null           \n",
      " matflag  | M              \n",
      " biryear  | 1976.0         \n",
      " dtaddto  | 10292016       \n",
      " gender   | F              \n",
      " insnum   | null           \n",
      " airline  | QF             \n",
      " admnum   | 9.495387003E10 \n",
      " fltno    | 00011          \n",
      " visatype | B1             \n",
      "-RECORD 1------------------\n",
      " cicid    | 5748518.0      \n",
      " i94yr    | 2016.0         \n",
      " i94mon   | 4.0            \n",
      " i94cit   | 245.0          \n",
      " i94res   | 438.0          \n",
      " i94port  | LOS            \n",
      " arrdate  | 20574.0        \n",
      " i94mode  | 1.0            \n",
      " i94addr  | NV             \n",
      " depdate  | 20591.0        \n",
      " i94bir   | 32.0           \n",
      " i94visa  | 1.0            \n",
      " count    | 1.0            \n",
      " dtadfile | 20160430       \n",
      " visapost | SYD            \n",
      " occup    | null           \n",
      " entdepa  | G              \n",
      " entdepd  | O              \n",
      " entdepu  | null           \n",
      " matflag  | M              \n",
      " biryear  | 1984.0         \n",
      " dtaddto  | 10292016       \n",
      " gender   | F              \n",
      " insnum   | null           \n",
      " airline  | VA             \n",
      " admnum   | 9.495562283E10 \n",
      " fltno    | 00007          \n",
      " visatype | B1             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
