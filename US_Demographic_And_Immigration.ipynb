{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### US Demographic and Immigration Data\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The goal of this project is to construct a pipeline for building a data lake in S3 with Spark for immigration data in the US. Three data sources are used:\n",
    "\n",
    "* I94 Immigration Data\n",
    "* USA Airport Codes\n",
    "* Country Code Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import pandas\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import udf, monotonically_increasing_id\n",
    "from pyspark.sql import types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://udacity-data/capstone/immigration/sas_data/, s3a://udacity-data/capstone/immigration/I94_SAS_LABELS_Descriptions.SAS\n",
      "s3a://udacity-data/capstone/airport/airport-codes_csv.csv\n",
      "s3a://udacity-data/capstone/demographic/us-cities-demographics.csv\n",
      "s3a://udacity-data/capstone/output/\n",
      "s3a://udacity-data/capstone/country_codes/country_codes_i94_clean.csv, s3a://udacity-data/capstone/country_codes/country_codes.csv\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read_file(open('config.cfg'))\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "bucket = config['S3']['BUCKET']\n",
    "immigration_path = bucket + config['S3']['IMMIGRATION_DATA']\n",
    "immigration_labels = bucket + config['S3']['IMMIGRATION_LABELS']\n",
    "airport_path = bucket + config['S3']['AIRPORT_CODES_DATA']\n",
    "city_path = bucket + config['S3']['CITY_DATA']\n",
    "output_path = bucket + config['S3']['OUTPUT']\n",
    "country_i94_path = bucket + config['S3']['COUNTRY_I94_DATA']\n",
    "country_path = bucket + config['S3']['COUNTRY_DATA']\n",
    "\n",
    "print(f'{immigration_path}, {immigration_labels}')\n",
    "print(f'{airport_path}')\n",
    "print(f'{city_path}')\n",
    "print(f'{output_path}')\n",
    "print(f'{country_i94_path}, {country_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON']='/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']='/usr/bin/python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.74:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6ba4ce0ef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.config('spark.jars.packages', \n",
    "                                    'org.apache.hadoop:hadoop-aws:2.7.0').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",\n",
    "                                     os.environ['AWS_ACCESS_KEY_ID'])\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \n",
    "                                     os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time():\n",
    "    n = datetime.now()\n",
    "\n",
    "    timestamp = f\"\"\"\n",
    "    {n.year}-{n.month}-{n.day}_{n.hour}-{n.minute}-{n.second}-{n.microsecond}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data = spark.read.parquet(immigration_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " cicid    | 5748517.0      \n",
      " i94yr    | 2016.0         \n",
      " i94mon   | 4.0            \n",
      " i94cit   | 245.0          \n",
      " i94res   | 438.0          \n",
      " i94port  | LOS            \n",
      " arrdate  | 20574.0        \n",
      " i94mode  | 1.0            \n",
      " i94addr  | CA             \n",
      " depdate  | 20582.0        \n",
      " i94bir   | 40.0           \n",
      " i94visa  | 1.0            \n",
      " count    | 1.0            \n",
      " dtadfile | 20160430       \n",
      " visapost | SYD            \n",
      " occup    | null           \n",
      " entdepa  | G              \n",
      " entdepd  | O              \n",
      " entdepu  | null           \n",
      " matflag  | M              \n",
      " biryear  | 1976.0         \n",
      " dtaddto  | 10292016       \n",
      " gender   | F              \n",
      " insnum   | null           \n",
      " airline  | QF             \n",
      " admnum   | 9.495387003E10 \n",
      " fltno    | 00011          \n",
      " visatype | B1             \n",
      "-RECORD 1------------------\n",
      " cicid    | 5748518.0      \n",
      " i94yr    | 2016.0         \n",
      " i94mon   | 4.0            \n",
      " i94cit   | 245.0          \n",
      " i94res   | 438.0          \n",
      " i94port  | LOS            \n",
      " arrdate  | 20574.0        \n",
      " i94mode  | 1.0            \n",
      " i94addr  | NV             \n",
      " depdate  | 20591.0        \n",
      " i94bir   | 32.0           \n",
      " i94visa  | 1.0            \n",
      " count    | 1.0            \n",
      " dtadfile | 20160430       \n",
      " visapost | SYD            \n",
      " occup    | null           \n",
      " entdepa  | G              \n",
      " entdepd  | O              \n",
      " entdepu  | null           \n",
      " matflag  | M              \n",
      " biryear  | 1984.0         \n",
      " dtaddto  | 10292016       \n",
      " gender   | F              \n",
      " insnum   | null           \n",
      " airline  | VA             \n",
      " admnum   | 9.495562283E10 \n",
      " fltno    | 00007          \n",
      " visatype | B1             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_data = spark.read.csv(airport_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | -74.93360137939453, 40.07080078125 \n",
      "-RECORD 1------------------------------------------\n",
      " ident        | 00AA                               \n",
      " type         | small_airport                      \n",
      " name         | Aero B Ranch Airport               \n",
      " elevation_ft | 3435                               \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-KS                              \n",
      " municipality | Leoti                              \n",
      " gps_code     | 00AA                               \n",
      " iata_code    | null                               \n",
      " local_code   | 00AA                               \n",
      " coordinates  | -101.473911, 38.704022             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_i94_data = spark.read.csv(country_i94_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " _c0                    | 0           \n",
      " i94cit_clean           | 582         \n",
      " i94_country_name_clean | MEXICO      \n",
      " iso_country_code_clean | 484         \n",
      "-RECORD 1-----------------------------\n",
      " _c0                    | 1           \n",
      " i94cit_clean           | 236         \n",
      " i94_country_name_clean | AFGHANISTAN \n",
      " iso_country_code_clean | 4           \n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_i94_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data = spark.read.csv(country_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------\n",
      " name                     | Afghanistan     \n",
      " alpha-2                  | AF              \n",
      " alpha-3                  | AFG             \n",
      " country-code             | 004             \n",
      " iso_3166-2               | ISO 3166-2:AF   \n",
      " region                   | Asia            \n",
      " sub-region               | Southern Asia   \n",
      " intermediate-region      | null            \n",
      " region-code              | 142             \n",
      " sub-region-code          | 034             \n",
      " intermediate-region-code | null            \n",
      "-RECORD 1-----------------------------------\n",
      " name                     | Åland Islands   \n",
      " alpha-2                  | AX              \n",
      " alpha-3                  | ALA             \n",
      " country-code             | 248             \n",
      " iso_3166-2               | ISO 3166-2:AX   \n",
      " region                   | Europe          \n",
      " sub-region               | Northern Europe \n",
      " intermediate-region      | null            \n",
      " region-code              | 150             \n",
      " sub-region-code          | 154             \n",
      " intermediate-region-code | null            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data = spark.read.option('delimiter', ';').csv(city_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " City                   | Silver Spring      \n",
      " State                  | Maryland           \n",
      " Median Age             | 33.8               \n",
      " Male Population        | 40601              \n",
      " Female Population      | 41862              \n",
      " Total Population       | 82463              \n",
      " Number of Veterans     | 1562               \n",
      " Foreign-born           | 30908              \n",
      " Average Household Size | 2.6                \n",
      " State Code             | MD                 \n",
      " Race                   | Hispanic or Latino \n",
      " Count                  | 25924              \n",
      "-RECORD 1------------------------------------\n",
      " City                   | Quincy             \n",
      " State                  | Massachusetts      \n",
      " Median Age             | 41.0               \n",
      " Male Population        | 44129              \n",
      " Female Population      | 49500              \n",
      " Total Population       | 93629              \n",
      " Number of Veterans     | 4147               \n",
      " Foreign-born           | 32935              \n",
      " Average Household Size | 2.39               \n",
      " State Code             | MA                 \n",
      " Race                   | White              \n",
      " Count                  | 58723              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Define the Data Model\n",
    "### Conceptual Data Model\n",
    "##### Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "The idea behind this pipeline is to combine different sources of data for the following purposes:\n",
    "\n",
    "* Track immigration by airport in the USA\n",
    "* Store details about each of those airports\n",
    "* Store details about the countries that USA gets immigrants from\n",
    "* Parse cities/states from airport data and store demographic data for those cities\n",
    "\n",
    "As such, the model would have a fact table and multiple dimension tables, with the fact table being the immigration data itself. This data would hold records to:\n",
    "\n",
    "* Uniquely identify each arrival\n",
    "* Hold mappings to dimensions for airport and country\n",
    "* Hold additional data about the entry like visa type\n",
    "\n",
    "The airpot, city and country dimension tables would contain additional information, not limited to the state/city the airports are in.\n",
    "\n",
    "For further reference, a schema diagram is included in the documentation.\n",
    "\n",
    "\n",
    "##### Mapping Out Data Pipelines\n",
    "\n",
    "\n",
    "1. Save initial datasets/sources to S3 buckets\n",
    "2. Load datasets into memory with Spark\n",
    "3. Create dimension tables for airports, countries, states\n",
    "4. Create the fact table for immigration, hence creating a mapping between each immigration record and its airport and country\n",
    "5. Perform data quality checks\n",
    "6. Save data as .parquet to S3 buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building out the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `countries` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- i94cit_clean: string (nullable = true)\n",
      " |-- i94_country_name_clean: string (nullable = true)\n",
      " |-- iso_country_code_clean: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_i94_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- alpha-2: string (nullable = true)\n",
      " |-- alpha-3: string (nullable = true)\n",
      " |-- country-code: string (nullable = true)\n",
      " |-- iso_3166-2: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- sub-region: string (nullable = true)\n",
      " |-- intermediate-region: string (nullable = true)\n",
      " |-- region-code: string (nullable = true)\n",
      " |-- sub-region-code: string (nullable = true)\n",
      " |-- intermediate-region-code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_i94_data.createOrReplaceTempView('country_i94_data')\n",
    "country_data.createOrReplaceTempView('country_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_table = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "    i.i94cit_clean AS i94_country_id,\n",
    "    i.iso_country_code_clean AS iso_country_id,\n",
    "    c.name AS country_name,\n",
    "    c.region AS region,\n",
    "    c.`sub-region` AS sub_region,\n",
    "    c.`alpha-2` AS iso_country_alpha\n",
    "    FROM country_data AS c\n",
    "    JOIN country_i94_data AS i ON\n",
    "        i.iso_country_code_clean = c.`country-code`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+------------+--------+--------------------+-----------------+\n",
      "|i94_country_id|iso_country_id|country_name|  region|          sub_region|iso_country_alpha|\n",
      "+--------------+--------------+------------+--------+--------------------+-----------------+\n",
      "|           529|           660|    Anguilla|Americas|Latin America and...|               AI|\n",
      "|           532|           533|       Aruba|Americas|Latin America and...|               AW|\n",
      "+--------------+--------------+------------+--------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_table.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_table.createOrReplaceTempView('countries_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing airports_table to s3a://udacity-data/capstone/output/countries_table_2020-6-20_20-29-37-832176..\n"
     ]
    }
   ],
   "source": [
    "countries_table_path = f'{output_path}countries_table_{get_time()}'\n",
    "print(f'Writing airports_table to {countries_table_path}..')\n",
    "# countries_table.write.parquet(countries_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `cities` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def make_iso_region(state):\n",
    "    return f'US-{state}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data.createOrReplaceTempView('city_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_table = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        City AS city,\n",
    "        `State Code` AS state,\n",
    "        `Median Age` AS median_age,\n",
    "        `Male Population` AS population_m,\n",
    "        `Female Population` AS population_f,\n",
    "        `Total Population` AS population_total,\n",
    "        `Race` AS race\n",
    "    FROM city_data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_table = cities_table.withColumn('iso_region', make_iso_region('state'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_table_path = f'{output_path}cities_table_{get_time()}'\n",
    "print(f'Writing airports_table to {cities_table_path}..')\n",
    "# cities_table.write.parquet(cities_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `airports` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_data.createOrReplaceTempView('airport_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_table = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        a.name AS airport_name,\n",
    "        c.iso_country_id AS iso_country_id,\n",
    "        a.local_code AS local_code,\n",
    "        a.Iata_code AS iata_code,\n",
    "        a.coordinates AS coordinates\n",
    "    FROM airport_data AS a\n",
    "    JOIN countries_table AS c\n",
    "    ON (c.iso_country_alpha = a.iso_country)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_table = airport_table.withColumn('airport_id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+----------+---------+-------------------+----------+\n",
      "|        airport_name|iso_country_id|local_code|iata_code|        coordinates|airport_id|\n",
      "+--------------------+--------------+----------+---------+-------------------+----------+\n",
      "|Clayton J Lloyd I...|           660|      null|      AXA|-63.055099, 18.2048|         0|\n",
      "|Queen Beatrix Int...|           533|      null|      AUA|-70.015198, 12.5014|         1|\n",
      "+--------------------+--------------+----------+---------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_table.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing airports_table to s3a://udacity-data/capstone/output/airport_table_2020-6-20_19-57-49-239188..\n"
     ]
    }
   ],
   "source": [
    "airport_table_path = f'{output_path}airport_table_{get_time()}'\n",
    "print(f'Writing airports_table to {airport_table_path}..')\n",
    "# airport_table.write.parquet(airport_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project write-up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credit to the following authors for additional datasets on countries and country codes:**\n",
    "\n",
    "* https://github.com/lukes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credit to the following authors for inspiration for the project and additional datasets:**\n",
    "\n",
    "* https://github.com/jukkakansanaho"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
