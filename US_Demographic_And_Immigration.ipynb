{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### US Demographic and Immigration Data\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The goal of this project is to construct a pipeline for building a data lake in S3 with Spark for demographic and immigration data in the US. Three data sources are used:\n",
    "\n",
    "* I94 Immigration Data\n",
    "* USA Airport Codes\n",
    "* USA City Demographic Data\n",
    "* Country Code Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import pandas\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://udacity-data/capstone/immigration/sas_data/, s3a://udacity-data/capstone/immigration/I94_SAS_LABELS_Descriptions.SAS\n",
      "s3a://udacity-data/capstone/airport/airport-codes_csv.csv\n",
      "s3a://udacity-data/capstone/demographic/us-cities-demographics.csv\n",
      "s3a://udacity-data/capstone/output\n",
      "s3a://udacity-data/capstone/country_codes/country_codes_i94_clean.csv, s3a://udacity-data/capstone/country_codes/country_codes.csv\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read_file(open('config.cfg'))\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "bucket = config['S3']['BUCKET']\n",
    "immigration_path = bucket + config['S3']['IMMIGRATION_DATA']\n",
    "immigration_labels = bucket + config['S3']['IMMIGRATION_LABELS']\n",
    "airport_path = bucket + config['S3']['AIRPORT_CODES_DATA']\n",
    "city_path = bucket + config['S3']['CITY_DATA']\n",
    "output_path = bucket + config['S3']['OUTPUT']\n",
    "country_i94_path = bucket + config['S3']['COUNTRY_I94_DATA']\n",
    "country_path = bucket + config['S3']['COUNTRY_DATA']\n",
    "\n",
    "print(f'{immigration_path}, {immigration_labels}')\n",
    "print(f'{airport_path}')\n",
    "print(f'{city_path}')\n",
    "print(f'{output_path}')\n",
    "print(f'{country_i94_path}, {country_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON']='/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']='/usr/bin/python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.74:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6ba4ce0ef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.config('spark.jars.packages', \n",
    "                                    'org.apache.hadoop:hadoop-aws:2.7.0').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",\n",
    "                                     os.environ['AWS_ACCESS_KEY_ID'])\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \n",
    "                                     os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data = spark.read.parquet(immigration_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " cicid    | 5748517.0      \n",
      " i94yr    | 2016.0         \n",
      " i94mon   | 4.0            \n",
      " i94cit   | 245.0          \n",
      " i94res   | 438.0          \n",
      " i94port  | LOS            \n",
      " arrdate  | 20574.0        \n",
      " i94mode  | 1.0            \n",
      " i94addr  | CA             \n",
      " depdate  | 20582.0        \n",
      " i94bir   | 40.0           \n",
      " i94visa  | 1.0            \n",
      " count    | 1.0            \n",
      " dtadfile | 20160430       \n",
      " visapost | SYD            \n",
      " occup    | null           \n",
      " entdepa  | G              \n",
      " entdepd  | O              \n",
      " entdepu  | null           \n",
      " matflag  | M              \n",
      " biryear  | 1976.0         \n",
      " dtaddto  | 10292016       \n",
      " gender   | F              \n",
      " insnum   | null           \n",
      " airline  | QF             \n",
      " admnum   | 9.495387003E10 \n",
      " fltno    | 00011          \n",
      " visatype | B1             \n",
      "-RECORD 1------------------\n",
      " cicid    | 5748518.0      \n",
      " i94yr    | 2016.0         \n",
      " i94mon   | 4.0            \n",
      " i94cit   | 245.0          \n",
      " i94res   | 438.0          \n",
      " i94port  | LOS            \n",
      " arrdate  | 20574.0        \n",
      " i94mode  | 1.0            \n",
      " i94addr  | NV             \n",
      " depdate  | 20591.0        \n",
      " i94bir   | 32.0           \n",
      " i94visa  | 1.0            \n",
      " count    | 1.0            \n",
      " dtadfile | 20160430       \n",
      " visapost | SYD            \n",
      " occup    | null           \n",
      " entdepa  | G              \n",
      " entdepd  | O              \n",
      " entdepu  | null           \n",
      " matflag  | M              \n",
      " biryear  | 1984.0         \n",
      " dtaddto  | 10292016       \n",
      " gender   | F              \n",
      " insnum   | null           \n",
      " airline  | VA             \n",
      " admnum   | 9.495562283E10 \n",
      " fltno    | 00007          \n",
      " visatype | B1             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_data = spark.read.csv(airport_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | -74.93360137939453, 40.07080078125 \n",
      "-RECORD 1------------------------------------------\n",
      " ident        | 00AA                               \n",
      " type         | small_airport                      \n",
      " name         | Aero B Ranch Airport               \n",
      " elevation_ft | 3435                               \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-KS                              \n",
      " municipality | Leoti                              \n",
      " gps_code     | 00AA                               \n",
      " iata_code    | null                               \n",
      " local_code   | 00AA                               \n",
      " coordinates  | -101.473911, 38.704022             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data = spark.read.option('delimiter', ';').csv(city_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " City                   | Silver Spring      \n",
      " State                  | Maryland           \n",
      " Median Age             | 33.8               \n",
      " Male Population        | 40601              \n",
      " Female Population      | 41862              \n",
      " Total Population       | 82463              \n",
      " Number of Veterans     | 1562               \n",
      " Foreign-born           | 30908              \n",
      " Average Household Size | 2.6                \n",
      " State Code             | MD                 \n",
      " Race                   | Hispanic or Latino \n",
      " Count                  | 25924              \n",
      "-RECORD 1------------------------------------\n",
      " City                   | Quincy             \n",
      " State                  | Massachusetts      \n",
      " Median Age             | 41.0               \n",
      " Male Population        | 44129              \n",
      " Female Population      | 49500              \n",
      " Total Population       | 93629              \n",
      " Number of Veterans     | 4147               \n",
      " Foreign-born           | 32935              \n",
      " Average Household Size | 2.39               \n",
      " State Code             | MA                 \n",
      " Race                   | White              \n",
      " Count                  | 58723              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_i94_data = spark.read.csv(country_i94_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " _c0                    | 0           \n",
      " i94cit_clean           | 582         \n",
      " i94_country_name_clean | MEXICO      \n",
      " iso_country_code_clean | 484         \n",
      "-RECORD 1-----------------------------\n",
      " _c0                    | 1           \n",
      " i94cit_clean           | 236         \n",
      " i94_country_name_clean | AFGHANISTAN \n",
      " iso_country_code_clean | 4           \n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_i94_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data = spark.read.csv(country_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------\n",
      " name                     | Afghanistan     \n",
      " alpha-2                  | AF              \n",
      " alpha-3                  | AFG             \n",
      " country-code             | 004             \n",
      " iso_3166-2               | ISO 3166-2:AF   \n",
      " region                   | Asia            \n",
      " sub-region               | Southern Asia   \n",
      " intermediate-region      | null            \n",
      " region-code              | 142             \n",
      " sub-region-code          | 034             \n",
      " intermediate-region-code | null            \n",
      "-RECORD 1-----------------------------------\n",
      " name                     | Åland Islands   \n",
      " alpha-2                  | AX              \n",
      " alpha-3                  | ALA             \n",
      " country-code             | 248             \n",
      " iso_3166-2               | ISO 3166-2:AX   \n",
      " region                   | Europe          \n",
      " sub-region               | Northern Europe \n",
      " intermediate-region      | null            \n",
      " region-code              | 150             \n",
      " sub-region-code          | 154             \n",
      " intermediate-region-code | null            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Define the Data Model\n",
    "### Conceptual Data Model\n",
    "##### Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "The idea behind this pipeline is to combine different sources of data for the following purposes:\n",
    "\n",
    "* Track immigration by airport in the USA\n",
    "* Store details about each of those airports\n",
    "* Store details about the cities and states each of those airports are in, including demographic\n",
    "* Store details about the countries that USA gets immigrants from \n",
    "\n",
    "As such, the model would have a fact table and multiple dimension tables, with the fact table being the immigration data itself. This data would hold records to:\n",
    "\n",
    "* Uniquely identify each arrival\n",
    "* Hold mappings to dimensions for airport and country\n",
    "* Hold additional data about the entry like visa type\n",
    "\n",
    "The airpot and country dimension tables would contain additional information, not limited to the state/city the airports are in. \n",
    "\n",
    "For further reference, a schema diagram is included in the documentation.\n",
    "\n",
    "\n",
    "##### Mapping Out Data Pipelines\n",
    "\n",
    "\n",
    "1. Save initial datasets/sources to S3 buckets\n",
    "2. Load datasets into memory with Spark\n",
    "3. Create dimension tables for airports, countries, states\n",
    "4. Create the fact table for immigration, hence creating a mapping between each immigration record and its airport and country\n",
    "5. Perform data quality checks\n",
    "6. Save data as .parquet to S3 buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building out the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project write-up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credit to the following authors for additional datasets on countries and country codes:**\n",
    "\n",
    "* https://github.com/lukes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credit to the following authors for inspiration for the project and additional datasets:**\n",
    "\n",
    "* https://github.com/jukkakansanaho"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
